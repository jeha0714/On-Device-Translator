import streamlit as st
import speech_recognition as sr
from faster_whisper import WhisperModel
import argostranslate.translate
import os
import time
import queue
import threading
import io

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Custom Control Translator", page_icon="ğŸ›ï¸", layout="wide")
st.title("ğŸ›ï¸ On-Device Real-time Translator")

# 2. ìŠ¤íƒ€ì¼ ì •ì˜ (ë²„íŠ¼ ë° ìŠ¬ë¼ì´ë” ë””ìì¸)
st.markdown("""
<style>
    .main-container { display: flex; flex-direction: row; gap: 20px; }
    .box { 
        padding: 20px; 
        border-radius: 10px; 
        min-height: 300px;
        font-size: 22px;
        line-height: 1.6;
    }
    .en-box { background-color: #f8f9fa; border: 2px solid #dee2e6; color: #212529; }
    .ko-box { background-color: #e3f2fd; border: 2px solid #90caf9; color: #0d47a1; }
    .label { font-weight: bold; margin-bottom: 10px; font-size: 16px; color: #555; }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ë§ */
    div.stButton > button {
        width: 100%;
        height: 60px;
        font-size: 20px;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# 3. ëª¨ë¸ ë¡œë“œ (ì†ë„ ìµœì í™”: tiny.en)
@st.cache_resource
def load_models():
    whisper = WhisperModel("tiny.en", device="cpu", compute_type="int8", cpu_threads=8)
    return whisper

try:
    with st.spinner("AI ì—”ì§„ ë¡œë”© ì¤‘..."):
        model_whisper = load_models()
except Exception as e:
    st.error(f"ì˜¤ë¥˜: {e}")
    st.stop()

# 4. ìƒíƒœ ë³€ìˆ˜
if "is_listening" not in st.session_state: st.session_state.is_listening = False
if "audio_queue" not in st.session_state: st.session_state.audio_queue = queue.Queue()
if "stop_event" not in st.session_state: st.session_state.stop_event = threading.Event()
if "live_en" not in st.session_state: st.session_state.live_en = ""
if "live_ko" not in st.session_state: st.session_state.live_ko = ""
if "history" not in st.session_state: st.session_state.history = []

# --- ë…¹ìŒ ìŠ¤ë ˆë“œ ---
def record_thread(audio_queue, stop_event, energy_threshold):
    r = sr.Recognizer()
    # [í•µì‹¬] ì‚¬ìš©ìê°€ ìŠ¬ë¼ì´ë”ë¡œ ì„¤ì •í•œ ë¯¼ê°ë„ë¥¼ ì ìš©
    r.energy_threshold = energy_threshold 
    r.dynamic_energy_threshold = False # ìˆ˜ë™ ì œì–´ë¥¼ ìœ„í•´ ìë™ ì¡°ì ˆ ë”
    r.pause_threshold = 1.0

    try:
        with sr.Microphone() as source:
            accumulated_audio_data = io.BytesIO()
            speech_start_time = None
            has_speech = False
            MAX_SENTENCE_TIME = 10 

            while not stop_event.is_set():
                try:
                    audio_chunk = r.listen(source, timeout=1.0, phrase_time_limit=1.0)
                    accumulated_audio_data.write(audio_chunk.get_raw_data())

                    if not has_speech:
                        has_speech = True
                        speech_start_time = time.time()
                    
                    full_audio = sr.AudioData(accumulated_audio_data.getvalue(), source.SAMPLE_RATE, source.SAMPLE_WIDTH)
                    audio_queue.put((full_audio, False))

                    if time.time() - speech_start_time > MAX_SENTENCE_TIME:
                        audio_queue.put((full_audio, True))
                        accumulated_audio_data = io.BytesIO()
                        has_speech = False
                        speech_start_time = None
                
                except sr.WaitTimeoutError:
                    if has_speech:
                        full_audio = sr.AudioData(accumulated_audio_data.getvalue(), source.SAMPLE_RATE, source.SAMPLE_WIDTH)
                        audio_queue.put((full_audio, True))
                        accumulated_audio_data = io.BytesIO()
                        has_speech = False
                        speech_start_time = None
                    continue
                except:
                    break
    except:
        pass

# ==========================================
# 5. UI êµ¬ì„± (ì»¨íŠ¸ë¡¤ íŒ¨ë„)
# ==========================================

# [A] ë¯¼ê°ë„ ì¡°ì ˆ ìŠ¬ë¼ì´ë” (ìƒë‹¨ ë°°ì¹˜)
# 300(ì¡°ìš©í•œ ë°©) ~ 4000(ì‹œë„ëŸ¬ìš´ ê³³)
sensitivity = st.slider("ğŸšï¸ ë§ˆì´í¬ ë¯¼ê°ë„ (ë‚®ì„ìˆ˜ë¡ ì‘ì€ ì†Œë¦¬ë„ ì¡ìŒ)", min_value=100, max_value=2000, value=300, step=50, help="ì£¼ë³€ì´ ì‹œë„ëŸ¬ìš°ë©´ ê°’ì„ ë†’ì´ì„¸ìš”. ì¡°ìš©í•œ ê³³ì—ì„œëŠ” 300 ì •ë„ê°€ ì ë‹¹í•©ë‹ˆë‹¤.")

# [B] ì‹œì‘/ì¤‘ì§€ í†µí•© ë²„íŠ¼
# ìƒíƒœì— ë”°ë¼ ë²„íŠ¼ í…ìŠ¤íŠ¸ì™€ ê¸°ëŠ¥ì„ ë¶„ê¸°í•¨
if st.session_state.is_listening:
    # í˜„ì¬ ì‘ë™ ì¤‘ -> 'ì¤‘ì§€' ë²„íŠ¼ í‘œì‹œ
    if st.button("â¹ï¸ í†µì—­ ì¤‘ì§€ (Click to Stop)", type="primary"):
        st.session_state.is_listening = False
        st.session_state.stop_event.set()
        st.rerun()
else:
    # í˜„ì¬ ì •ì§€ë¨ -> 'ì‹œì‘' ë²„íŠ¼ í‘œì‹œ
    if st.button("â–¶ï¸ í†µì—­ ì‹œì‘ (Click to Start)"):
        st.session_state.is_listening = True
        st.session_state.history = []
        st.session_state.live_en = ""
        st.session_state.live_ko = ""
        st.session_state.stop_event.clear()
        with st.session_state.audio_queue.mutex: st.session_state.audio_queue.queue.clear()
        
        # ìŠ¬ë¼ì´ë” ê°’ì„ ìŠ¤ë ˆë“œë¡œ ì „ë‹¬
        t = threading.Thread(
            target=record_thread, 
            args=(st.session_state.audio_queue, st.session_state.stop_event, sensitivity), 
            daemon=True
        )
        t.start()
        st.rerun()

st.divider()

# [C] ë©”ì¸ í™”ë©´ (ì¢Œìš° ë¶„í• )
col1, col2 = st.columns(2)

with col1:
    st.markdown("<div class='label'>ğŸ‡ºğŸ‡¸ ENGLISH</div>", unsafe_allow_html=True)
    # íˆìŠ¤í† ë¦¬ + ë¼ì´ë¸Œ í…ìŠ¤íŠ¸ ê²°í•©
    full_text_en = "".join([h[0] + " " for h in st.session_state.history]) + f"**{st.session_state.live_en}**"
    st.markdown(f"<div class='box en-box'>{full_text_en}</div>", unsafe_allow_html=True)

with col2:
    st.markdown("<div class='label'>ğŸ‡°ğŸ‡· KOREAN</div>", unsafe_allow_html=True)
    full_text_ko = "".join([h[1] + " " for h in st.session_state.history]) + f"**{st.session_state.live_ko}**"
    st.markdown(f"<div class='box ko-box'>{full_text_ko}</div>", unsafe_allow_html=True)

# 6. ë©”ì¸ ë¡œì§ (íŒŒì¼ ì €ì¥ ë°©ì‹ + ê³ ì† ì„¤ì •)
if st.session_state.is_listening:
    if not st.session_state.audio_queue.empty():
        try:
            audio_data, is_final = st.session_state.audio_queue.get()
            
            # ì•ˆì „í•œ íŒŒì¼ ì €ì¥ ë°©ì‹
            temp_file = "temp_control.wav"
            with open(temp_file, "wb") as f: f.write(audio_data.get_wav_data())
            
            # Whisper ë³€í™˜
            segments, _ = model_whisper.transcribe(temp_file, beam_size=2, temperature=0.0, language="en")
            text_en = "".join([s.text for s in segments]).strip()
            
            if text_en:
                try:
                    text_ko = argostranslate.translate.translate(text_en, "en", "ko")
                except:
                    text_ko = "..."

                st.session_state.live_en = text_en
                st.session_state.live_ko = text_ko
                
                if is_final:
                    st.session_state.history.append((text_en, text_ko))
                    st.session_state.live_en = ""
                    st.session_state.live_ko = ""
            
            st.rerun()

        except Exception as e:
            st.rerun()
    else:
        time.sleep(0.05)
        st.rerun()
